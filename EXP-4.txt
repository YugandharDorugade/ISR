EXPERIMENT 4 â€” Precision and Recall
ğŸ§  Aim
To implement a program to calculate Precision and Recall for a sample input â€”
(given a query q1, its answer set A, and relevant documents Rq1).

ğŸ¯ Objective
To understand how to evaluate the performance of an Information Retrieval (IR) system.
To learn how precision and recall quantify search effectiveness.

ğŸ§© Background Context
When a user types a search query in an IR system (say â€œmachine learningâ€),
the system returns a list of documents.
Some are relevant (useful to the query), some are non-relevant (noise).
So, to measure how good the system is, we ask:
Out of all documents retrieved â†’ how many were actually useful? (Precision)
Out of all useful documents in the database â†’ how many were retrieved? (Recall)

âš–ï¸ The 4 Possible Categories of Documents
When you run a query on a database, every document falls into one of these 4 groups:

		Relevant		Not Relevant
Retrieved	âœ… True Positive (TP)	ğŸš« False Positive (FP)
Not Retrieved	âŒ False Negative (FN)	âœ… True Negative (TN)
We mostly focus on TP, FP, FN in IR.

ğŸ“˜ Definitions
Metric	Formula	Meaning
Precision (P)	P = TP / (TP + FP)	Out of all retrieved documents, how many were relevant
Recall (R)	R = TP / (TP + FN)	Out of all relevant documents, how many were retrieved

So:
Precision â†’ Accuracy of the retrieved set
Recall â†’ Completeness of the retrieved set

ğŸ”¢ Numerical Example
Suppose:
Total database = 100 docs
Relevant docs = 20
IR system retrieves 10 docs
Out of those 10, only 6 are relevant
So:
TP = 6 (relevant & retrieved)
FP = 4 (retrieved but irrelevant)
FN = 14 (relevant but not retrieved)

Precision= 6+4 / 6 = 0.6
Recall=6+14 / 6 = 0.3
âœ… Precision = 60%
âœ… Recall = 30%
â†’ System is precise but incomplete (retrieves good results, but misses many).

ğŸ” Precisionâ€“Recall Trade-off
If we make the search broader â†’ recall â†‘, precision â†“ (retrieve more docs, but many irrelevant).
If we make it stricter â†’ recall â†“, precision â†‘ (retrieve only a few, all relevant).
You can always get 100% recall by retrieving everything.
You can always get 100% precision by retrieving just one perfect result.
But you canâ€™t get both simultaneously.
Hence, a balance is needed.

ğŸ“‰ Precisionâ€“Recall Curve
When results are ranked (like in Google),
we calculate precision and recall at each rank position and plot them.
The better the algorithm â†’ the curve stays toward the top right (high precision & high recall).

âš™ï¸ Algorithm Steps
Input:
Set of relevant documents (Rq1)
Set of retrieved documents (A)

Compute:
Intersection of both sets â†’ relevant & retrieved

Calculate:
Precision = |Rq1 âˆ© A| / |A|
Recall = |Rq1 âˆ© A| / |Rq1|
Display precision and recall.

ğŸ’» Python-Style Pseudocode
# Relevant documents for query q1
Rq1 = {"D1", "D2", "D3", "D5", "D8"}
# Retrieved documents (answer set)
A   = {"D2", "D3", "D4", "D6", "D8"}

# Intersection (relevant & retrieved)
common = Rq1.intersection(A)

precision = len(common) / len(A)
recall = len(common) / len(Rq1)

print("Relevant & Retrieved:", common)
print("Precision =", round(precision, 2))
print("Recall =", round(recall, 2))

Output:
Relevant & Retrieved: {'D2', 'D3', 'D8'}
Precision = 0.6
Recall = 0.6


âœ… So this IR system retrieved 60% of relevant docs, and 60% of retrieved docs were actually relevant.

ğŸ’¬ Theory Summary
Concept	Meaning
Precision		Quality of retrieved documents
Recall			Quantity of relevant documents retrieved
TP (True Positive)	Docs both relevant and retrieved
FP (False Positive)	Retrieved but not relevant
FN (False Negative)	Relevant but not retrieved
Trade-off		More recall â†’ less precision (and vice versa)


-------------------------QNA----------------------------

1. Precision: The ratio of correctly predicted positive observations to the total predicted positives.
Formula: Precision = TP / (TP + FP)
2. Recall: The ratio of correctly predicted positive observations to all actual positives.
Formula: Recall = TP / (TP + FN)
3. When you increase recall: Precision usually decreases, as the model captures more positives but also more false positives.
4. Ideal case for precision and recall: Both equal to 1 (100%), meaning perfect classificationâ€”no false positives or false negatives.
5. Shape of the Precisionâ€“Recall curve: Typically a downward-sloping curve; precision decreases as recall increases.
6. True Positive (TP): Correctly predicted positive case (model predicts positive, and it actually is positive).
7. False Positive (FP): Incorrectly predicted positive case (model predicts positive, but itâ€™s actually negative).
8. Importance of precision and recall: They measure the modelâ€™s performance in imbalanced datasets and indicate the trade-off between false alarms and missed detections.
9. 100% recall with low precision: Yes, possibleâ€”if the model predicts all samples as positive, it catches all true positives but also many false positives.
10. When recall is unmeasurable: When there are no actual positive cases in the dataset (denominator = 0).